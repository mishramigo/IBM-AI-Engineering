{"cells": [{"metadata": {}, "cell_type": "code", "source": "!pip install ibmos2spark", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20201014072227-0000\nKERNEL_ID = f0d60a44-fa65-4e62-aa88-0e7924aff9ea\nCollecting ibmos2spark\n  Downloading ibmos2spark-1.0.1-py2.py3-none-any.whl (7.4 kB)\nInstalling collected packages: ibmos2spark\nSuccessfully installed ibmos2spark-1.0.1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import ibmos2spark\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-5119afa5-9f36-4561-8de3-537f498c07d2',\n    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n    'api_key': 'UdCSHHwYJp9QLwhI_Gt3F7rxwpCcfZ48WNZKvwQlD1Hk'\n}\n\nconfiguration_name = 'os_ab26667fc49e46aaa28ad8a08f0a62c0_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read.parquet(cos.url('hmp.parquet', 'scalablemachinelearningonbigdatau-donotdelete-pr-zvgztoifq8gdmt'))\ndf_data_1.show()\n", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# copying df_data_1 to df for simple usage\ndf = df_data_1", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# splitting in train and test data sets in 80:20\nsplits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# preprocessing steps\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Creating pipeline\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# training the model\nmodel = pipeline.fit(df_train)", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# predicting on training data\nprediction = model.transform(df_train)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# evaluation using MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n    \nbinEval.evaluate(prediction)", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "0.2068701648166835"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# Prediction on testdata\nprediction = model.transform(df_test)", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# evaluating test prediction\nbinEval.evaluate(prediction)", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "0.20553866033931537"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.10", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}