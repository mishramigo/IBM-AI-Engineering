{"cells": [{"metadata": {}, "cell_type": "code", "source": "!pip install ibmos2spark", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Collecting ibmos2spark\n  Downloading ibmos2spark-1.0.1-py2.py3-none-any.whl (7.4 kB)\nInstalling collected packages: ibmos2spark\nSuccessfully installed ibmos2spark-1.0.1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### Please upload hmp.parquet to object storage before running the next cell."}, {"metadata": {}, "cell_type": "code", "source": "import ibmos2spark\n# @hidden_cell\ncredentials = {\n    'endpoint': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'service_id': 'iam-ServiceId-5119afa5-9f36-4561-8de3-537f498c07d2',\n    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n    'api_key': 'UdCSHHwYJp9QLwhI_Gt3F7rxwpCcfZ48WNZKvwQlD1Hk'\n}\n\nconfiguration_name = 'os_ab26667fc49e46aaa28ad8a08f0a62c0_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read.parquet(cos.url('hmp.parquet', 'scalablemachinelearningonbigdatau-donotdelete-pr-zvgztoifq8gdmt'))\ndf_data_1.show()\n", "execution_count": 36, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df = df_data_1", "execution_count": 37, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# creating temporary query view\ndf.createOrReplaceTempView('df')", "execution_count": 38, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# feature engineering one additional column\ndf_energy = spark.sql('''\n\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label, class from df group by class\n\n'''\n)\n\ndf_energy.createOrReplaceTempView('df_energy')", "execution_count": 39, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_join = spark.sql('''\n\nselect * from df inner join df_energy on df.class = df_energy.class\n\n''')\ndf_join.show()", "execution_count": 40, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+-----------------+-----------+\n|  x|  y|  z|              source|      class|            label|      class|\n+---+---+---+--------------------+-----------+-----------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n+---+---+---+--------------------+-----------+-----------------+-----------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# vectorAssembler and normalizer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler = VectorAssembler(inputCols = ['x', 'y', 'z'],\n                                  outputCol = 'features')\nnormalizer = Normalizer(inputCol = 'features',\n                       outputCol = 'features_norm', p = 1.0)", "execution_count": 41, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import linearRegression\nfrom pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)", "execution_count": 42, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Pipeline\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [vectorAssembler, normalizer, lr])", "execution_count": 43, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# modeling\nmodel = pipeline.fit(df_join)", "execution_count": 45, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# predict\nprediction = model.transform(df_join)", "execution_count": 48, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# finding r2 score of the linear regression which is stage number 3 in stages so index = 2\nmodel.stages[2].summary.r2", "execution_count": 51, "outputs": [{"output_type": "execute_result", "execution_count": 51, "data": {"text/plain": "0.03259100556263628"}, "metadata": {}}]}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.10", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}